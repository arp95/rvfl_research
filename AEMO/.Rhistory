# normalize
w[t+1,]=w[t+1,]/sum(w[t+1,])
t=t+1
}
max_t=t-1
phi=0.5
# iterate
t=1
while (t<T){
idx<-sample(m,replace=TRUE,prob=w[t,])
tth_x=x[idx,]
tth_y=y[idx,]
# call weak learner, CART
model[[t]]=eval(parse(text=method))(tth_x,tth_y)
# getback to hypothesis
fit[t,]<-y_fit<-predict(model[[t]],x)
# adjusted error for each instance
ARE<-abs((y-y_fit)/y)
is_ARE_gt_phi=ARE>phi
is_ARE_le_phi=ARE<=phi
# calculate error of hypothesis
epsilon[t]=sum(w[t,is_ARE_gt_phi])
# calculate beta
beta[t]<-epsilon[t]^2
# update weight vector
w[t+1,is_ARE_le_phi]=w[t,is_ARE_le_phi]*beta[t]
w[t+1,is_ARE_gt_phi]=w[t,is_ARE_gt_phi]
# normalize
w[t+1,]=w[t+1,]/sum(w[t+1,])
t=t+1
}
max_t=t-1
source('C:/Users/renye/Baiduyun/R/work/AdaBoost/myAdaBoostR2.R')
max_t
Y<-fit[1:max_t,]
Y
delta<-pinv(Y)%*%y
?pinv
require(MASS)
delta<-pinv(Y)%*%y
delta<-ginv(Y)%*%y
ginv(Y)
y.inv<-ginv(Y)
dim(y.inv)
dim(y)
delta<-t(ginv(Y))%*%y
dim(delta)
dim(Y)
aggregated.fit<-t(Y)%*%delta
aggregated.fit
plot(aggregated.fit)
plot(aggregated.fit,type='l')
lines(y,col='red')
eye
?eye
t(Y)%*%Y
inv(t(Y)%*%Y+sigma*diag(m))
ginv(t(Y)%*%Y+sigma*diag(m))
t(ginv(t(Y)%*%Y+sigma*diag(m))%*%t(Y))
t(ginv(t(Y)%*%Y+sigma*diag(m))%*%t(Y))%*%y
aggregated.fit<-t(Y)%*%delta
lines(aggregated.fit,col='green')
m<-nrow(x)
n<-ncol(x)
y<-matrix(y,nrow=m)
T=maxIter # max iter
#initialize weight distr and error
epsilon<-beta<-D<-rep(NA,T)
fit<-e<-w<-array(NA,dim=c(T,m))
w[1,]=rep(1/m, m)
model<-list()
# iterate
errorcnt<-rep(0,T)
upfactor<-downfactor<-rep(NA,T)
BEM=0.5
D[1,]=rep(1/m, m)
epsilon<-beta<-rep(NA,T)
fit<-e<-D<-array(NA,dim=c(T,m))
D[1,]=rep(1/m, m)
idx<-sample(m,replace=TRUE,prob=D[t,])
tth_x=x[idx,]
t=1
idx<-sample(m,replace=TRUE,prob=D[t,])
tth_x=x[idx,]
tth_y=y[idx,]
# call weak learner, CART
model[[t]]=eval(parse(text=method))(tth_x,tth_y)
# getback to hypothesis
fit[t,]<-y_fit<-predict(model[[t]],x)
AE=abs(y_fit-y); # absolute error
# calculate error count AE>BEM
is_AE_gt_BEM=AE>BEM
View(is_AE_gt_BEM)
AE
BEM=0.25
is_AE_gt_BEM=AE>BEM
View(is_AE_gt_BEM)
errorcnt[t]=sum(is_AE_gt_BEM)
# calculate upfactor and downfactor
upfactor[t]=m/errorcnt[t]
downfactor[t]=1/upfactor[t]
is_AE_le_BEM=!is_AE_gt_BEM
D[t+1,is_AE_gt_BEM]=D[t,is_AE_gt_BEM]*upfactor[t]
D[t+1,is_AE_le_BEM]=D[t,is_AE_le_BEM]*downfactor[t]
# normalize
# normalize
D[t+1,]=D[t+1,]/sum(D[t+1,])
t=t+1
t=1
while (t<=T){
idx<-sample(m,replace=TRUE,prob=D[t,])
tth_x=x[idx,]
tth_y=y[idx,]
# call weak learner, CART
model[[t]]=eval(parse(text=method))(tth_x,tth_y)
# getback to hypothesis
fit[t,]<-y_fit<-predict(model[[t]],x)
AE=abs(y_fit-y) # absolute error
# calculate error count AE>BEM
is_AE_gt_BEM=AE>BEM
errorcnt[t]=sum(is_AE_gt_BEM)
# calculate upfactor and downfactor
upfactor[t]=m/errorcnt[t]
downfactor[t]=1/upfactor[t]
# update distr
is_AE_le_BEM=!is_AE_gt_BEM
D[t+1,is_AE_gt_BEM]=D[t,is_AE_gt_BEM]*upfactor[t]
D[t+1,is_AE_le_BEM]=D[t,is_AE_le_BEM]*downfactor[t]
# normalize
# normalize
D[t+1,]=D[t+1,]/sum(D[t+1,])
t=t+1
}
D[1,]
D[2,]
# iterate
t=1
while (t<=T){
idx<-sample(m,replace=TRUE,prob=D[t,])
tth_x=x[idx,]
tth_y=y[idx,]
# call weak learner, CART
model[[t]]=eval(parse(text=method))(tth_x,tth_y)
# getback to hypothesis
fit[t,]<-y_fit<-predict(model[[t]],x)
AE=abs(y_fit-y) # absolute error
# calculate error count AE>BEM
is_AE_gt_BEM=AE>BEM
errorcnt[t]=sum(is_AE_gt_BEM)
# calculate upfactor and downfactor
upfactor[t]=m/errorcnt[t]
downfactor[t]=1/upfactor[t]
# update distr
is_AE_le_BEM=!is_AE_gt_BEM
D[t+1,is_AE_gt_BEM]=D[t,is_AE_gt_BEM]*upfactor[t]
D[t+1,is_AE_le_BEM]=D[t,is_AE_le_BEM]*downfactor[t]
# normalize
D[t+1,]=D[t+1,]/sum(D[t+1,])
t=t+1
}
t
idx<-sample(m,replace=TRUE,prob=D[t,])
D[29,]
D[28,]
upfactor[28]
sum(is_AE_gt_BEM)
require(e1071)
?svm
source('C:/Users/renye/Baiduyun/R/work/Method/svm_fn.R')
my_cv_svm
my_cv_svm()
cod<-read.table('cod.data',header=TRUE)
View(cod)
?t.test
?fredman.test
?fredmann.test
?friedman.test
friedman.test(cod[,2:6])
friedman.test(cod[,3:6])
cod[,3:6]
a<-cod[,3:6]
dimnames(a)
fredman.test(a)
friedman.test(a)
friedman.test(a,groups=colnames(a))
friedman.test(a,groups=colnames(a),blocks=rownames(a))
RoundingTimes <-
matrix(c(5.40, 5.50, 5.55,
5.85, 5.70, 5.75,
5.20, 5.60, 5.50,
5.55, 5.50, 5.40,
5.90, 5.85, 5.70,
5.45, 5.55, 5.60,
5.40, 5.40, 5.35,
5.45, 5.50, 5.35,
5.25, 5.15, 5.00,
5.85, 5.80, 5.70,
5.25, 5.20, 5.10,
5.65, 5.55, 5.45,
5.60, 5.35, 5.45,
5.05, 5.00, 4.95,
5.50, 5.50, 5.40,
5.45, 5.55, 5.50,
5.55, 5.55, 5.35,
5.45, 5.50, 5.55,
5.50, 5.45, 5.25,
5.65, 5.60, 5.40,
5.70, 5.65, 5.55,
6.30, 6.30, 6.25),
nrow = 22,
byrow = TRUE,
dimnames = list(1 : 22,
c("Round Out", "Narrow Angle", "Wide Angle")))
as.matrix(a)
friedman.test(as.matrix(a))
matplot(a,type='l')
library(e1071)
?svm
# create data
x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)
# estimate model and predict input values
m   <- svm(x, y)
new <- predict(m, x)
# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)
plot(x,y)
lines(x,new,col=2)
lines(x,new-0.1,col=3,type='dashed')
lines(x,new-0.1,col=3,lty='dashed')
lines(x,new-0.5,col=3,lty='dashed')
lines(x,new+0.5,col=3,lty='dashed')
m
names(m)
m$SV
m$index
length(m$index)
plot(x,y)
points(m$SV,m$index,col='red')
points(m$index,m$SV,col='red')
x
points(x[m$index],m$SV,col='red')
points(x[m$index],y[m$index],col='red')
lines(x,new,col='green')
lines(x,new-0.1,col='green')
lines(x,new+0.1,col='green')
plot(x,y)
points(x[m$index],y[m$index],col='red')
lines(x,new,col='green',lwd=1.5)
lines(x,new,col='blue',lwd=1.5)
lines(x,new+0.1,col='green',lty='dashed')
lines(x,new-0.1,col='green',lty='dashed')
lines(x,new-0.1,col='green',lty='dashed',lwd=1.5)
lines(x,new+0.1,col='green',lty='dashed',lwd=1.5)
lines(x,new+0.1,col='darkgreen',lty='dashed',lwd=1.5)
lines(x,new-0.1,col='darkgreen',lty='dashed',lwd=1.5)
library(caret)
names(getModelInfo())
getModelInfo()$svmRadial$type
urlfile <-'https://raw.githubusercontent.com/hadley/fueleconomy/master/data-raw/vehicles.csv'
x <- getURL(urlfile, ssl.verifypeer = FALSE)
library(RCurl)
urlfile <-'https://raw.githubusercontent.com/hadley/fueleconomy/master/data-raw/vehicles.csv'
x <- getURL(urlfile, ssl.verifypeer = FALSE)
vehicles <- read.csv(textConnection(x))
vehicles <- vehicles[names(vehicles)[1:24]]
vehicles <- data.frame(lapply(vehicles, as.character), stringsAsFactors=FALSE)
vehicles <- data.frame(lapply(vehicles, as.numeric))
vehicles[is.na(vehicles)] <- 0
vehicles$cylinders <- ifelse(vehicles$cylinders == 6, 1,0)
prop.table(table(vehicles$cylinders))
set.seed(1234)
vehicles <- vehicles[sample(nrow(vehicles)),]
split <- floor(nrow(vehicles)/3)
split
ensembleData <- vehicles[0:split,]
blenderData <- vehicles[(split+1):(split*2),]
testingData <- vehicles[(split*2+1):nrow(vehicles),]
ensembleData <- vehicles[0:split,]
blenderData <- vehicles[(split+1):(split*2),]
testingData <- vehicles[(split*2+1):nrow(vehicles),]
labelName <- 'cylinders'
predictors <- names(ensembleData)[names(ensembleData) != labelName]
myControl <- trainControl(method='cv', number=3, returnResamp='none')
test_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm', trControl
=myControl)
preds <- predict(object=test_model, testingData[,predictors])
library(pROC)
auc <- roc(testingData[,labelName], preds)
print(auc$auc)
library(pROC)
install.packages('pROC')
preds <- predict(object=test_model, testingData[,predictors])
library(pROC)
auc <- roc(testingData[,labelName], preds)
print(auc$auc)
roc(testingData[,labelName], preds)
plot(auc)
model_gbm <- train(ensembleData[,predictors], ensembleData[,labelName], method='gbm', trContro
l=myControl)
model_rpart <- train(ensembleData[,predictors], ensembleData[,labelName], method='rpart', trCo
ntrol=myControl)
model_treebag <- train(ensembleData[,predictors], ensembleData[,labelName], method='treebag',
trControl=myControl)
blenderData$gbm_PROB <- predict(object=model_gbm, blenderData[,predictors])
blenderData$rf_PROB <- predict(object=model_rpart, blenderData[,predictors])
blenderData$treebag_PROB <- predict(object=model_treebag, blenderData[,predictors])
testingData$gbm_PROB <- predict(object=model_gbm, testingData[,predictors])
testingData$rf_PROB <- predict(object=model_rpart, testingData[,predictors])
testingData$treebag_PROB <- predict(object=model_treebag, testingData[,predictors])
model_rpart <- train(ensembleData[,predictors], ensembleData[,labelName], method='rpart', trCo
ntrol=myControl)
model_gbm <- train(ensembleData[,predictors], ensembleData[,labelName], method='gbm', trControl=myControl)
model_rpart <- train(ensembleData[,predictors], ensembleData[,labelName], method='rpart', trCo									 ntrol=myControl)
model_treebag <- train(ensembleData[,predictors], ensembleData[,labelName], method='treebag',
trControl=myControl)
model_rpart <- train(ensembleData[,predictors], ensembleData[,labelName], method='rpart',
trControl=myControl)
# see how each individual model performed on its own
auc <- roc(testingData[,labelName], testingData$gbm_PROB )
print(auc$auc) # Area under the curve: 0.9893
auc <- roc(testingData[,labelName], testingData$rf_PROB )
print(auc$auc) # Area under the curve: 0.958
auc <- roc(testingData[,labelName], testingData$treebag_PROB )
print(auc$auc) # Area under the curve: 0.9734
# run a final model to blend all the probabilities together
predictors <- names(blenderData)[names(blenderData) != labelName]
final_blender_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm', trControl=my
Control)
# See final prediction and AUC of blended ensemble
preds <- predict(object=final_blender_model, testingData[,predictors])
auc <- roc(testingData[,labelName], preds)
print(auc$auc) # Area under the curve: 0.9922
# run a final model to blend all the probabilities together
predictors <- names(blenderData)[names(blenderData) != labelName]
final_blender_model <- train(blenderData[,predictors], blenderData[,labelName], method='gbm',
trControl=myControl)
# See final prediction and AUC of blended ensemble
preds <- predict(object=final_blender_model, testingData[,predictors])
auc <- roc(testingData[,labelName], preds)
print(auc$auc) # Area under the curve: 0.9922
test_model <- train(blenderData[,predictors], blenderData[,labelName], method='svmRadial', trControl
=myControl)
preds <- predict(object=test_model, testingData[,predictors])
library(pROC)
auc <- roc(testingData[,labelName], preds)
print(auc$auc)
plot(auc)
require(foreavh)
require(foreach)
?foreach
?dopar
?doParallel
?unlist
a<-array(rnorm(100,1,0.1),c(10,2,5))
apply(a,3,sum)
a
apply(a,2,sum)
apply(a,1,sum)
?apply
apply(a,1,`+`)
apply(a,3,`+`)
apply(a,2,`+`)
apply(a,1,`+`)
a[1,]
a[1,,]
apply(a,c(1,2),sum)
sum(a[1,,])
a[1,1,]
sum(a[1,1,])
apply(a,1,`+`)
a[1,,]
apply(a,2,`+`)
apply(a,3,`+`)
a[,,1]
apply(a,1,`+`)
apply(a,c(1,2),rbind)
apply(a,c(1,2),cbind)
cbind(a[,,])
cbind(a)
apply(a,3,cbind)
apply(a,c(2,3),cbind)
apply(a,c(1,2),cbind)
unlist(a)
library(abind)
install.packages('abind')
library(abind)
abind(a,along=3)
abind(a)
a=array(runif(10,0,1),c(10,2,5))
apply(a,c(1,2),cbind)
apply(a,1,cbind)
a
a=array(runif(10,1,0.1),c(10,2,5))
a=array(rnorm(10,1,0.1),c(10,2,5))
plot(a)
a=array(rnorm(100,1,0.1),c(10,2,5))
apply(a,1,cbind)
a
apply(a,1,rbind)
library(emd)
library(EMD)
?emd
### Empirical Mode Decomposition
ndata <- 3000
tt2 <- seq(0, 9, length=ndata)
xt2 <- sin(pi * tt2) + sin(2* pi * tt2) + sin(6 * pi * tt2)  + 0.5 * tt2
try <- emd(xt2, tt2, boundary="wave")
?emd
emd(xt2,tt2,plot.IMF=TRUE)
emd(xt2,tt2,plot.imf=TRUE)
ts<-xt2
ext.ts<-extrema(ts) #{EMD}
# 1) find min, max and t.min, t.max
t.max.1<-ext.ts$maxindex[1,1]
t.min.1<-ext.ts$minindex[1,1]
t.max.n<-ext.ts$maxindex[nrow(ext.ts$maxindex),1]
t.min.n<-ext.ts$minindex[nrow(ext.ts$minindex),1]
max.1<-ts[t.max.1]
max.n<-ts[t.max.n]
min.1<-ts[t.min.1]
min.n<-ts[t.min.n]
plot(ts,type='o')
plot(ts,type='ol')
plot(ts,type='l')
points(c(t.max.1,t.min.1,t.max.n,t.min.n),c(max.1,min.1,max.n,min.n))
points(c(t.max.1,t.min.1,t.max.n,t.min.n),c(max.1,min.1,max.n,min.n),col='red')
t.min.0=-t.max.1
min.0=min.1
t.max.0=-t.min.1
max.0=max.1
# 3) find t.max.n1, t.min.n1, max.n1, min.n1
t.min.n1=2*length(ts)-t.max.n
min.n1=min.n
t.max.n1=2*length(ts)-t.min.n
max.n1=max.n
# 4) interpolate
if(t.min.0<=t.max.0){
ts.beginning<-approx(c(t.min.0,t.max.0,0),c(min.0,max.0,ts[1]),n=abs(t.min.0-0))
extra.beginning=-t.min.0
}else{
ts.beginning<-approx(c(t.max.0,t.min.0,0),c(max.0,min.0,ts[1]),n=abs(t.max.0-0))
extra.beginning=-t.max.0
}
if(t.min.n1<=t.max.n1){
ts.end<-approx(c(length(ts),t.min.n1,t.max.n1),c(ts[length(ts)],min.n1,max.n1),n=abs(t.min.n1-length(ts)))
extra.end=t.min.n1-length(ts)
}else{
ts.end<-approx(c(length(ts),t.max.n1,t.min.n1),c(ts[length(ts)],max.n1,min.n1),n=abs(t.max.n1-length(ts)))
extra.end=t.max.n1-length(ts)
}
#browser()
ts.new<-c(ts.beginning$y,ts,ts.end$y)
lines(ts.new)
plot(ts.new)
plot(ts.new,type='l')
org.index<-1:length(ts)+extra.beginning
lines(org.index,ts,col='red')
library(forecast)
?arima.sim
ts<-arima.sim(list(order=c(24,0,0),ar=rep(0.5,24)),n=500)
ts<-arima.sim(list(order=c(24,0,0),ar=rep(1/24,24)),n=500)
ts<-arima.sim(list(order=c(1,0,0),ar=0.7,seasonal=c(1,1,0)),n=500)
acf(ts)
plot(acf)
plot(ts)
ts<-arima.sim(list(order=c(1,0,0),ar=0.7),n=500)
acf(ts)
pacf(ts)
ts<-arima.sim(list(order=c(1,0,0),ar=0.7,seasonal=list(order=c(1,0,0),period=12)),n=500)
pacf(ts)
plot(ts)
auto.arima(ts)
?simulate.Arima
setwd("C:/Users/renye/Baiduyun/R/dataset/AEMO")
load<-read.csv('DATA2013_NSW1.csv')
head(load)
load.data<-loda[,'RRP']
load.data<-load[,'RRP']
plot(load.data)
plot(load.data[1:100])
plot(load.data[1:480])
load.data<-loda[,'TOT']
load.data<-loda$T
load.data<-load$T
plot(load.data)
a<-load.data[1:1000]
library(forecast)
acf(a)
pacf(a)
pacf(a,48)
pacf(a,96)
acf(a,96)
diff.a<-diff(a)
acf(diff.a)
acf(diff.a,96)
diff48.a<-diff(a,48)
acf(diff48.a,96)
pacf(diff48.a,96)
plot(diff48.a)
plot(diff.a)
plot(diff.a,type='l')
decompose(diff.a)
decompose(a)
plot(a,type='l')
plot(diff.a,type='l')
diff1and48.a<-diff(diff.a,48)
acf(diff1and48.a,96)
acf(diff48.a,96)
plot(diff1and48.a,type='l')
